{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ddec744-6f54-4d6b-96ab-0033fdf13b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.4.0\n",
      "  Using cached transformers-4.4.0-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers==4.4.0) (1.19.0)\n",
      "Requirement already satisfied: sacremoses in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers==4.4.0) (0.0.49)\n",
      "Requirement already satisfied: filelock in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers==4.4.0) (3.6.0)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers==4.4.0) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers==4.4.0) (4.62.3)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers==4.4.0) (2022.3.15)\n",
      "Requirement already satisfied: packaging in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from transformers==4.4.0) (21.3)\n",
      "Requirement already satisfied: six in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from sacremoses->transformers==4.4.0) (1.16.0)\n",
      "Requirement already satisfied: joblib in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from sacremoses->transformers==4.4.0) (0.14.1)\n",
      "Requirement already satisfied: click in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from sacremoses->transformers==4.4.0) (8.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->transformers==4.4.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->transformers==4.4.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->transformers==4.4.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from requests->transformers==4.4.0) (2019.11.28)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from packaging->transformers==4.4.0) (3.0.7)\n",
      "\u001b[31mERROR: ratsnlp 1.0.5 has requirement transformers==4.10.0, but you'll have transformers 4.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: kobert 0.2.3 has requirement transformers>=4.8.1, but you'll have transformers 4.4.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.18.0\n",
      "    Uninstalling transformers-4.18.0:\n",
      "      Successfully uninstalled transformers-4.18.0\n",
      "Successfully installed tokenizers-0.10.3 transformers-4.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc95ef3-0f9a-4464-8e63-c28cf0c5aeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import time\n",
    "# start = time.time()  # 시작 시간 저장\n",
    " \n",
    " \n",
    "# # 작업 코드\n",
    " \n",
    " \n",
    "# print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b4d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0731a8ce-9cc5-4e43-b8ed-880947b18bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer import \n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2d89f1e-e033-4ce7-b276-930835034f59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bert model 불러온 후 저장된 모델 상태 적용\n",
    "model2 =  BertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=5)\n",
    "# model2.load_state_dict(torch.load('bert_model_ver2_st_0505.pt'))\n",
    "model2.load_state_dict(torch.load('bert_model_ver2(77)_st_0505.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39150ce1-b72e-4ab5-85f6-f90242977415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device 적용\n",
    "device = torch.device(\"cuda:0\")\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "241502a7-1c5b-4b11-9f4f-8810762dbdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 변환\n",
    "def convert_input_data(sentences):\n",
    "\n",
    "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # 입력 토큰의 최대 시퀀스 길이\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    # 토큰을 숫자 인덱스로 변환\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    \n",
    "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # 어텐션 마스크 초기화\n",
    "    attention_masks = []\n",
    "\n",
    "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # 데이터를 파이토치의 텐서로 변환\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ceba6b-f965-4a45-9994-5dad3edbd969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 문장 테스트\n",
    "def test_sentences(sentences):\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model2.eval()\n",
    "\n",
    "    # 문장을 입력 데이터로 변환\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "\n",
    "    # 데이터를 GPU에 넣음\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "            \n",
    "    # 그래디언트 계산 안함\n",
    "    with torch.no_grad():     \n",
    "        # Forward 수행\n",
    "        outputs = model2(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    # 로스 구함\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # CPU로 데이터 이동\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6dea38-a008-42fa-a1fa-660f78a2fcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰를 입력해 주세요. : \n",
      "저렴한 가격으로 잘 샀어요 무난하게 사용하기 좋네요\n",
      "---------------------\n",
      "⭐⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "매트리스가 견고하고 탄탄하며 좋네요\n",
      "---------------------\n",
      "⭐⭐⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "침대 배송도 빠르고 친절하게 잘 설치해 주셔서 잘 사용하고 있습니다 수납공간도 넓어서 좋아요\n",
      "---------------------\n",
      "⭐⭐⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "사무실에서 피곤할 때 사용하려고 샀는데 실밥이 너무 많이 나와 있어요 ㅠㅠ\n",
      "---------------------\n",
      "⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "위에서 움직일 때 흔들림이 있어요\n",
      "---------------------\n",
      "⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "높이가 약간 낮은 감이 있어서 조금 아쉬워요\n",
      "---------------------\n",
      "⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "깔끔하고 맘에 들어요\n",
      "---------------------\n",
      "⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "디자인 색상 만족합니다\n",
      "---------------------\n",
      "⭐⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "괜찮아요 작은방에 크기가 딱\n",
      "---------------------\n",
      "⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "판매원은 친절하고 좋은데 제품은 보통\n",
      "---------------------\n",
      "⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "디자인 색상 만족합니다\n",
      "---------------------\n",
      "⭐⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "저렴하게 사무실용으로 괜찮아요\n",
      "---------------------\n",
      "⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "튼튼해서 좋아요 많은 구매\n",
      "---------------------\n",
      "⭐⭐⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "좋습니다 저는 만족합니다\n",
      "---------------------\n",
      "⭐⭐⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "제품은 가격 대비 만족할 만한 퀄리티입니다\n",
      "---------------------\n",
      "⭐⭐⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "소파는 가격 대비 좋아요\n",
      "---------------------\n",
      "⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "디자인이 이뻐서 구매했어요 사진이랑은 똑같아요\n",
      "---------------------\n",
      "⭐⭐⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "디자인이 이뻐서 구매했어요 사진이랑 똑같아요\n",
      "---------------------\n",
      "⭐⭐⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "전체적으로 약간 작은 듯합니다\n",
      "---------------------\n",
      "⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "하이마트 몰은 문의전화 연결이 너무 힘들고 풀 편합니다 택배 배송도 다른 곳으로 배송돼서 번거롭기도 했고요\n",
      "---------------------\n",
      "⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "튼튼하고 좋은데 움직일 때마다 삐걱삐 걱 소리가 나요ㅠㅠ\n",
      "---------------------\n",
      "⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "매트리스가 별로입니다\n",
      "---------------------\n",
      "⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "위에서 움직일 때 흔들림이 있어요\n",
      "---------------------\n",
      "⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "사무실에서 피곤할 때 사용하려고 샀는데 실밥이 너무 많이 나와 있어요 ㅠㅠ\n",
      "---------------------\n",
      "⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "쿠션감은 딱딱한데 소파용으로 그냥 쓰고 있어요\n",
      "---------------------\n",
      "⭐⭐⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n",
      "보름 만에 도착 너무 오래 걸림\n",
      "---------------------\n",
      "⭐\n",
      "-----------------------------------------------------------------------------------------------\n",
      "리뷰를 입력해 주세요. : \n"
     ]
    }
   ],
   "source": [
    "# sentences = pd.read_csv('review_test_data_total_ver2.csv', encoding = 'utf-8')\n",
    "sentences_re = []\n",
    "while True:\n",
    "    i = 0\n",
    "    start = time.time()  # 시작 시간 저장\n",
    "    for review in sentences['review']:\n",
    "        if i >2000:\n",
    "            break\n",
    "        print('리뷰를 입력해 주세요. : ')\n",
    "        review = input()\n",
    "        print('---------------------')\n",
    "        logits = test_sentences([review])\n",
    "#         print(review)\n",
    "#         print(logits)\n",
    "#         print(np.argmax(logits))\n",
    "        if np.argmax(logits) == 0 :\n",
    "#             print(review)\n",
    "            print(\"⭐\")\n",
    "            print('-----------------------------------------------------------------------------------------------')\n",
    "            sentences_re.append(1)\n",
    "        elif np.argmax(logits) == 1 :\n",
    "#             print(i)\n",
    "            print(\"⭐⭐\")\n",
    "            print('-----------------------------------------------------------------------------------------------')\n",
    "            sentences_re.append(2)\n",
    "        elif np.argmax(logits) == 2 :\n",
    "#             print(i)\n",
    "            print(\"⭐⭐⭐\")\n",
    "            print('-----------------------------------------------------------------------------------------------')\n",
    "            sentences_re.append(3)\n",
    "        elif np.argmax(logits) == 3 :\n",
    "#             print(i)\n",
    "            print(\"⭐⭐⭐⭐\")\n",
    "            print('-----------------------------------------------------------------------------------------------')\n",
    "            sentences_re.append(4)\n",
    "        else :\n",
    "#             print(i)\n",
    "            print(\"⭐⭐⭐⭐⭐\")\n",
    "            print('-----------------------------------------------------------------------------------------------')\n",
    "            sentences_re.append(5)\n",
    "        i+=1\n",
    "\n",
    "#     print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930ec72-afdf-4a04-891d-fb21580e735c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b01670-c954-4792-bc01-8a909472e429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "conda-env-azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
